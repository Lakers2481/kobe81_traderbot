# INTEGRATION RECOMMENDATIONS: Top 10 Prioritized Fixes

**Date:** 2026-01-08
**Audit Type:** Quant-Level Due Diligence
**Scope:** External Resources + Kobe Wiring Verification

---

## EXECUTIVE SUMMARY

**Priority Framework:**
1. **HIGH PRIORITY:** Fix wiring issues (data loss, defaults) - NO NEW DEPENDENCIES
2. **MEDIUM PRIORITY:** Add high-value external components (FinGPT, TradeMaster)
3. **LOW PRIORITY:** Nice-to-have improvements (UI upgrades, dev workflow)

**Critical Finding:** Kobe creates **99 enrichment fields** but only uses ~10 for position sizing. The rest are lost because **DecisionPacket is never saved**. Fix this FIRST before adding new components.

---

## FIX #1: Save DecisionPacket for Learning

**Priority:** ðŸ”´ CRITICAL (Week 1, Day 1-2)
**Category:** Wiring Fix (No New Dependencies)

### Problem
- `core/decision_packet.py` defines comprehensive `DecisionPacket` schema with 99 fields
- Schema includes: MarketSnapshot, IndicatorSnapshot, MLSnapshot, RiskSnapshot, SignalSnapshot
- **NEVER CALLED** in production execution path (`run_paper_trade.py`, `runner.py`)
- Enriched data (ML confidence, sentiment, regime, etc.) is computed but NOT persisted for learning

### Impact
- Cannot learn from full decision context
- Cannot replay decisions for debugging
- Cannot analyze which enrichment fields actually improve outcomes
- Violates reproducibility requirement for quant interview readiness

### Files to Modify
- `scripts/run_paper_trade.py:750-848` (add DecisionPacket creation)
- `core/decision_packet.py:150-448` (already defined, just needs wiring)

### Change

**Location:** `scripts/run_paper_trade.py` (after order placement, line 750)

```python
# BEFORE (current):
audit_data = {
    'decision_id': rec.decision_id,
    'symbol': sym,
    'side': side,
    'qty': max_qty,
    # ... minimal fields only
}
append_audit_block(audit_data, config_pin)

# AFTER (fixed):
from core.decision_packet import create_decision_packet, DecisionPacket

# Create comprehensive decision packet
packet = create_decision_packet(
    symbol=sym,
    ohlcv=data[data['symbol'] == sym].iloc[-1].to_dict(),  # Latest bar
    indicators={
        'rsi2': row.get('rsi2', 0),
        'ibs': row.get('ibs', 0),
        'atr14': row.get('atr', 0),
        # ... all indicators
    },
    signal=row.to_dict(),  # Full EnrichedSignal (99 fields)
    ml_predictions=[
        {'model': 'ml_meta', 'confidence': row.get('ml_meta_conf', 0), 'direction': side},
        {'model': 'lstm', 'confidence': row.get('lstm_direction', 0), 'magnitude': row.get('lstm_magnitude', 0)},
        {'model': 'ensemble', 'confidence': row.get('ensemble_conf', 0)},
        {'model': 'markov', 'pi_up': row.get('markov_pi_up', 0), 'p_up_today': row.get('markov_p_up_today', 0)},
    ],
    risk_checks={
        'policy_gate_passed': True,  # If we got here, it passed
        'kill_zone': can_trade_now(),
        'quality_score': row.get('quality_score', 0),
        'quality_tier': row.get('quality_tier', 'UNKNOWN'),
    },
    strategy_params={
        'strategy': row.get('strategy', 'IBS_RSI'),
        'entry_price': limit_px,
        'stop_loss': stop_loss,
        'take_profit': take_profit,
        'time_stop_bars': row.get('time_stop_bars', 7),
    },
    decision='BUY' if side == 'long' else 'SELL',
    reason=f"Kelly={kelly_pct:.2%}, Regime={regime}, VIX={vix_level:.0f}, Conf={final_confidence:.0%}"
)

# Save to state/decisions/
packet_path = f"state/decisions/{packet.packet_id}.json"
packet.save(packet_path)

# Also log to audit chain (existing code)
audit_data = packet.to_audit_block()  # Convert to audit format
append_audit_block(audit_data, config_pin)
```

### Verification

**Unit Test:** `tests/unit/test_decision_packet.py`
```python
def test_decision_packet_saved_on_order():
    """Verify DecisionPacket is created and saved when order is placed."""
    # Run paper trade with 1 signal
    run_paper_trade(cap=10, signals=[mock_signal])

    # Check packet was saved
    packet_files = list(Path("state/decisions/").glob("DEC_*.json"))
    assert len(packet_files) == 1, "DecisionPacket not saved"

    # Load packet and verify 99 enrichment fields present
    with open(packet_files[0]) as f:
        packet_data = json.load(f)

    assert 'ml_predictions' in packet_data, "ML predictions missing"
    assert len(packet_data['ml_predictions']) == 4, "Should have 4 ML models"
    assert 'signal' in packet_data, "Signal data missing"
    assert len(packet_data['signal']) >= 99, "Missing enrichment fields"
```

**Integration Test:**
```bash
# Run paper trade and verify packet saved
python scripts/run_paper_trade.py --cap 10 --skip-wf-check
# Check: state/decisions/ directory should have DEC_*.json files
ls -lh state/decisions/
# Expected: 1 file per trade with ~50KB size (99 fields)
```

**Runtime Assertion:**
```python
# Add assertion in run_paper_trade.py after packet.save()
assert Path(packet_path).exists(), f"DecisionPacket not saved: {packet_path}"
assert Path(packet_path).stat().st_size > 10000, "DecisionPacket too small (missing fields)"
```

### Rollback
- Feature flag: `--save-decision-packet` (default: True)
- Disable with `--no-decision-packet` if storage becomes issue
- Old behavior: Only audit_data logged (minimal fields)

### ROI
- **Enables learning** from full decision context
- **Reproducibility** for quant interview
- **Debugging** capabilities (replay decisions)
- **NO COST** - just wiring existing infrastructure

---

## FIX #2: Replace VADER Sentiment with FinGPT

**Priority:** ðŸ”´ HIGH (Week 1, Day 5)
**Category:** External Integration (High-Value Replacement)

### Problem
- `altdata/sentiment.py` uses VADER (rule-based, 2014 era)
- VADER not fine-tuned for financial news
- Sentiment defaults to **0.5 (neutral)** when cache empty (~30% of scans)
- Sentiment has 20% weight in final confidence: `conf = 0.8Ã—ML + 0.2Ã—sentiment`

### Impact
- Inaccurate sentiment scores
- 30% of signals get neutral 0.5 default (no real sentiment data)
- 20% of confidence score is based on outdated model

### Files to Modify
- `altdata/sentiment.py:45-120` (replace VADER with FinGPT)
- `requirements.txt` (add transformers, torch)

### Change

**Location:** `altdata/sentiment.py`

```python
# BEFORE (VADER):
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()
scores = analyzer.polarity_scores(text)
compound = scores['compound']  # Range: [-1, 1]

# AFTER (FinGPT):
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

class FinGPTSentimentAnalyzer:
    def __init__(self):
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "FinGPT/fingpt-sentiment_llama2-13b_lora"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "FinGPT/fingpt-sentiment_llama2-13b_lora"
        )

    def analyze(self, text: str) -> float:
        """Analyze sentiment of financial text."""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)

        # Get sentiment logits (positive, negative, neutral)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)[0]

        # Convert to compound score [-1, 1]
        # positive: index 0, negative: index 1, neutral: index 2
        compound = (probs[0] - probs[1]).item()  # positive - negative

        return compound  # Range: [-1, 1]

# Global analyzer instance
_analyzer = None

def get_analyzer():
    global _analyzer
    if _analyzer is None:
        _analyzer = FinGPTSentimentAnalyzer()
    return _analyzer

def analyze_sentiment_fingpt(text: str) -> float:
    """Analyze sentiment using FinGPT fine-tuned model."""
    if not text or len(text.strip()) < 10:
        return 0.0  # Neutral for empty/short text

    analyzer = get_analyzer()
    return analyzer.analyze(text)
```

### Verification

**Unit Test:** `tests/altdata/test_sentiment_fingpt.py`
```python
def test_fingpt_sentiment_analysis():
    """Verify FinGPT sentiment returns valid scores."""
    # Positive news
    pos_text = "Company reports record earnings, beats expectations by 20%"
    pos_score = analyze_sentiment_fingpt(pos_text)
    assert pos_score > 0.3, f"Expected positive sentiment, got {pos_score}"

    # Negative news
    neg_text = "Company misses earnings, cuts guidance, layoffs announced"
    neg_score = analyze_sentiment_fingpt(neg_text)
    assert neg_score < -0.3, f"Expected negative sentiment, got {neg_score}"

    # Neutral news
    neu_text = "Company files quarterly report with SEC"
    neu_score = analyze_sentiment_fingpt(neu_text)
    assert -0.2 < neu_score < 0.2, f"Expected neutral sentiment, got {neu_score}"
```

**A/B Test:**
```python
# Compare VADER vs FinGPT on same news corpus
news_corpus = load_polygon_news(start="2025-01-01", end="2025-12-31")

vader_scores = [analyze_sentiment_vader(article) for article in news_corpus]
fingpt_scores = [analyze_sentiment_fingpt(article) for article in news_corpus]

# Verify FinGPT has higher correlation with next-day returns
corr_vader = correlation(vader_scores, next_day_returns)
corr_fingpt = correlation(fingpt_scores, next_day_returns)

assert corr_fingpt > corr_vader, f"FinGPT should have higher correlation: {corr_fingpt} vs {corr_vader}"
```

### Rollback
- Feature flag: `--sentiment-model` (default: "fingpt", fallback: "vader")
- Config: `sentiment_provider: "fingpt"` in config/base.yaml
- Fallback: If FinGPT fails to load, fall back to VADER with warning

### Cost
- **Model:** Free (HuggingFace hosted)
- **Fine-tuning:** $300/month for continuous LoRA fine-tuning (optional)
- **Latency:** 200-500ms per symbol (batch processing recommended)

### ROI
- **Better sentiment accuracy** (fine-tuned on financial news 2015-2023)
- **No more 0.5 defaults** (always get real sentiment score)
- **Improved confidence scores** (20% weight on better sentiment)

---

## FIX #3: Add TradeMaster PRUDEX Benchmark

**Priority:** ðŸ”´ HIGH (Week 1, Day 3-4)
**Category:** External Integration (Validation)

### Problem
- RL agent exists (`ml_advanced/rl_agent/`) but **no industry benchmark**
- Cannot compare PPO agent vs 15+ other RL algorithms
- No regime-specific performance validation
- Trade history not in standardized format

### Impact
- Cannot validate RL agent quality
- No proof it's better than simpler baselines
- Quant interview would ask: "How does this compare to SAC/TD3/DQN?"

### Files to Modify
- `scripts/export_trademaster.py` (new file)
- `scripts/run_trademaster_eval.py` (new file)
- `requirements.txt` (add trademaster)

### Change

**New File:** `scripts/export_trademaster.py`

```python
"""Export Kobe trade history to TradeMaster format."""
import pandas as pd
from pathlib import Path

def export_trade_history_to_trademaster(
    trade_log_path: str = "logs/trades.csv",
    output_path: str = "trademaster_exports/kobe_history.csv",
):
    """Convert Kobe trade log to TradeMaster TRADE_SCHEMA."""
    trades = pd.read_csv(trade_log_path)

    # TradeMaster expects columns: date, tic, close, open, high, low, volume, pnl
    trademaster_df = pd.DataFrame({
        'date': pd.to_datetime(trades['timestamp']),
        'tic': trades['symbol'],
        'close': trades['exit_price'],
        'open': trades['entry_price'],
        'high': trades['entry_price'] + trades['mfe'],  # Max Favorable Excursion
        'low': trades['entry_price'] + trades['mae'],   # Max Adverse Excursion
        'volume': trades['volume'],
        'pnl': trades['pnl'],
    })

    # Save
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    trademaster_df.to_csv(output_path, index=False)
    print(f"[OK] Exported {len(trademaster_df)} trades to {output_path}")
    return output_path

if __name__ == "__main__":
    export_trade_history_to_trademaster()
```

**New File:** `scripts/run_trademaster_eval.py`

```python
"""Run TradeMaster PRUDEX-Compass evaluation on Kobe trades."""
from trademaster.evaluation import PRUDEX
from trademaster.agents import PPO, SAC, TD3, DQN, A2C

def run_prudex_evaluation(
    trade_history_path: str = "trademaster_exports/kobe_history.csv",
    benchmark: str = "SPY",
):
    """Run PRUDEX-Compass benchmark on Kobe trade history."""

    # Load Kobe trades
    kobe_trades = pd.read_csv(trade_history_path)

    # Run PRUDEX evaluation
    prudex = PRUDEX(
        trades=kobe_trades,
        benchmark_symbol=benchmark,
        transaction_cost_bps=5,  # Alpaca fee
        regimes=["BULL", "BEAR", "NEUTRAL"],  # Match Kobe HMM regimes
    )

    results = prudex.evaluate()

    # Print results
    print("=" * 80)
    print("TRADEMASTER PRUDEX-COMPASS EVALUATION")
    print("=" * 80)
    print(f"Overall Sharpe Ratio: {results['sharpe']:.2f}")
    print(f"Overall Calmar Ratio: {results['calmar']:.2f}")
    print(f"Overall Sortino Ratio: {results['sortino']:.2f}")
    print(f"Max Drawdown: {results['max_drawdown']:.2%}")
    print()
    print("Regime-Specific Performance:")
    for regime in ["BULL", "BEAR", "NEUTRAL"]:
        print(f"  {regime}:")
        print(f"    Sharpe: {results[f'{regime.lower()}_sharpe']:.2f}")
        print(f"    Win Rate: {results[f'{regime.lower()}_win_rate']:.1%}")

    # Save results
    results_df = pd.DataFrame([results])
    results_df.to_csv("trademaster_exports/prudex_results.csv", index=False)
    print()
    print("[OK] Results saved to trademaster_exports/prudex_results.csv")

    return results

if __name__ == "__main__":
    run_prudex_evaluation()
```

### Verification

**Integration Test:**
```bash
# Export Kobe trade history
python scripts/export_trademaster.py

# Run PRUDEX evaluation
python scripts/run_trademaster_eval.py

# Expected output:
# ================================================================================
# TRADEMASTER PRUDEX-COMPASS EVALUATION
# ================================================================================
# Overall Sharpe Ratio: 1.85
# Overall Calmar Ratio: 2.10
# Overall Sortino Ratio: 2.35
# Max Drawdown: -8.50%
#
# Regime-Specific Performance:
#   BULL:
#     Sharpe: 2.10
#     Win Rate: 68.5%
#   BEAR:
#     Sharpe: 1.20
#     Win Rate: 55.0%
#   NEUTRAL:
#     Sharpe: 1.50
#     Win Rate: 60.0%
```

**Comparison Test:**
```python
# Compare Kobe PPO vs TradeMaster baseline algorithms
def compare_against_baselines():
    algorithms = ['PPO', 'SAC', 'TD3', 'DQN', 'A2C']
    results = {}

    for algo in algorithms:
        agent = get_trademaster_agent(algo)
        metrics = agent.backtest(env=trading_env)
        results[algo] = metrics

    # Kobe should beat at least 3 out of 5 baselines
    kobe_sharpe = results['PPO']['sharpe']  # Kobe uses PPO
    baselines = ['SAC', 'TD3', 'DQN', 'A2C']
    beaten = sum(1 for algo in baselines if kobe_sharpe > results[algo]['sharpe'])

    assert beaten >= 3, f"Kobe PPO should beat at least 3 baselines, only beat {beaten}"
```

### Rollback
- No rollback needed - this is pure validation/benchmarking
- Does not affect production execution

### Cost
- **FREE** - TradeMaster is open-source (Apache 2.0 license)

### ROI
- **Validation** of RL agent quality
- **Quant interview readiness** (can cite industry benchmark)
- **Continuous improvement** (monthly PRUDEX reports)

---

## FIX #4: Rebuild RAG with Hands-On LLM Patterns

**Priority:** âš ï¸ MEDIUM (Week 3-4)
**Category:** External Integration (RAG Rebuild)

### Problem
- RAG infrastructure exists (`cognitive/symbol_rag.py`, `vector_memory.py`, `rag_evaluator.py`)
- **ZERO production usage** - only imported in deprecated `cognitive_brain.py`
- No embeddings generated in production flow
- No RAG evaluation (faithfulness, relevance)

### Impact
- Cannot leverage historical trade knowledge
- Cannot provide evidence-based trade thesis
- Brain recommendations not grounded in data

### Files to Modify
- `cognitive/symbol_rag.py` (rebuild with evaluation)
- `pipelines/unified_signal_enrichment.py` (wire into enrichment)
- `requirements.txt` (add sentence-transformers)

### Change

**Location:** `cognitive/symbol_rag.py` (rebuild)

```python
"""Symbol-aware RAG with evaluation patterns from Hands-On LLM."""
from sentence_transformers import SentenceTransformer
from typing import Dict, List, Optional
import chromadb
from cognitive.rag_evaluator import evaluate_rag_response

class SymbolRAG:
    def __init__(self, collection_name: str = "symbol_knowledge"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = chromadb.PersistentClient(path="state/cognitive/vector_db")
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def index_trade_history(self, trades: List[Dict]):
        """Index historical trades for retrieval."""
        documents = []
        metadatas = []
        ids = []

        for i, trade in enumerate(trades):
            # Create semantic document from trade
            doc = f"""
            Symbol: {trade['symbol']}
            Strategy: {trade['strategy']}
            Entry: ${trade['entry_price']:.2f}, Exit: ${trade['exit_price']:.2f}
            Result: {trade['outcome']} ({trade['pnl_pct']:.1%})
            Context: {trade['reason']}
            Regime: {trade['regime']}
            """
            documents.append(doc)
            metadatas.append(trade)
            ids.append(f"trade_{i}")

        # Generate embeddings and index
        embeddings = self.model.encode(documents)
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )

    def query(self, question: str, n_results: int = 5) -> Dict:
        """Query RAG and return response with evaluation."""
        # Retrieve relevant trades
        query_embedding = self.model.encode([question])
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=n_results
        )

        # Build response
        response = {
            'question': question,
            'retrieved_documents': results['documents'][0],
            'metadatas': results['metadatas'][0],
            'distances': results['distances'][0],
        }

        # Evaluate RAG quality (Hands-On LLM pattern)
        eval_score = evaluate_rag_response(
            response=response,
            faithfulness_threshold=0.7,
            relevance_threshold=0.6,
        )

        response['evaluation'] = eval_score

        # Stand down if quality too low
        if eval_score.faithfulness < 0.7:
            response['recommendation'] = "STAND_DOWN"
            response['reason'] = "RAG quality below threshold"
        else:
            response['recommendation'] = "PROCEED"

        return response
```

**Wire into Enrichment:** `pipelines/unified_signal_enrichment.py`

```python
# Add RAG stage (line 600-650)
@dataclass
class RAGEnrichment(EnrichmentStage):
    name: str = "rag_retrieval"

    def enrich(self, signal: EnrichedSignal) -> EnrichedSignal:
        """Query RAG for similar historical trades."""
        rag = get_symbol_rag()

        query = f"Historical trades for {signal.symbol} after {signal.streak_length} down days"
        rag_response = rag.query(query, n_results=5)

        if rag_response['recommendation'] == 'STAND_DOWN':
            signal.rag_quality_low = True
            signal.rag_confidence = 0.0
        else:
            signal.rag_quality_low = False
            signal.rag_confidence = rag_response['evaluation'].faithfulness
            signal.rag_similar_trades = len(rag_response['retrieved_documents'])

        return signal

# Register RAG stage
ENRICHMENT_STAGES.append(RAGEnrichment())
```

### Verification

**Unit Test:**
```python
def test_rag_evaluation():
    """Verify RAG evaluation prevents low-quality responses."""
    rag = SymbolRAG()

    # Index some trade history
    trades = [
        {'symbol': 'AAPL', 'strategy': 'IBS_RSI', 'outcome': 'WIN', 'pnl_pct': 0.05},
        {'symbol': 'AAPL', 'strategy': 'IBS_RSI', 'outcome': 'LOSS', 'pnl_pct': -0.02},
    ]
    rag.index_trade_history(trades)

    # Query for relevant trade
    response = rag.query("How does AAPL perform after 5 down days?")

    # Should have evaluation scores
    assert 'evaluation' in response
    assert 'faithfulness' in response['evaluation']
    assert 'relevance' in response['evaluation']

    # Should stand down if quality low
    if response['evaluation'].faithfulness < 0.7:
        assert response['recommendation'] == 'STAND_DOWN'
```

### Rollback
- Feature flag: `--enable-rag` (default: False for now)
- Disable with `--no-rag` if latency too high

### Cost
- **Model:** Free (sentence-transformers open-source)
- **Storage:** ~100MB for 10K trades
- **Latency:** 100-200ms per query

### ROI
- **Evidence-based decisions** (grounded in historical data)
- **Knowledge boundary** detection (stand down when uncertain)
- **Explainability** (cite similar historical trades)

---

## FIX #5: Make Kill Zone Gate Authoritative

**Priority:** âš ï¸ MEDIUM (Week 2, Day 1-2)
**Category:** Wiring Fix (Safety Enhancement)

### Problem
- Kill Zone Gate exists (`risk/kill_zone_gate.py`) and correctly blocks 9:30-10:00 AM, lunch chop
- Returns `can_trade: bool` but **NOT enforced at broker boundary**
- Only used in status checks, not order placement
- **Gap:** Orders could theoretically be placed during blocked zones

### Impact
- Risk of trading during amateur hour (9:30-10:00 AM)
- Risk of trading during lunch chop (11:30-14:00 PM)
- Not quant-grade safety (should enforce like PolicyGate)

### Files to Modify
- `execution/broker_alpaca.py:48-157` (add kill zone check to decorator)
- `risk/kill_zone_gate.py:270-293` (add exception on violation)

### Change

**Location:** `risk/kill_zone_gate.py`

```python
# BEFORE (advisory):
def can_trade_now() -> bool:
    """Simple check if trading is allowed right now."""
    gate = get_kill_zone_gate()
    status = gate.check_can_trade()
    return status.can_trade  # Returns boolean only

# AFTER (authoritative):
class KillZoneViolationError(Exception):
    """Raised when attempting to trade during blocked kill zone."""
    pass

def check_kill_zone_enforcement(allow_exits: bool = False) -> None:
    """Check kill zone and raise exception if blocked."""
    gate = get_kill_zone_gate()
    status = gate.check_can_trade()

    if not status.can_trade:
        if allow_exits and status.zone in ['close', 'power_hour']:
            # Allow exits during close/power hour
            return
        else:
            raise KillZoneViolationError(
                f"Kill zone active: {status.zone} ({status.reason})"
            )

def require_valid_kill_zone(func):
    """Decorator to enforce kill zone at broker boundary."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        check_kill_zone_enforcement()  # Raises if blocked
        return func(*args, **kwargs)
    return wrapper
```

**Location:** `execution/broker_alpaca.py`

```python
# Add to decorator chain (line 48-157)
from risk.kill_zone_gate import require_valid_kill_zone

@require_no_kill_switch
@require_policy_gate
@require_valid_kill_zone  # NEW: Enforce kill zone
def place_order(order: BrokerOrder) -> BrokerOrderResult:
    """Place order with full safety checks."""
    # ... existing code ...
```

### Verification

**Unit Test:**
```python
def test_kill_zone_blocks_orders():
    """Verify kill zone decorator blocks orders during amateur hour."""
    from datetime import time
    from unittest.mock import patch

    # Mock time to be 9:45 AM (opening range block)
    with patch('risk.kill_zone_gate.datetime') as mock_dt:
        mock_dt.now.return_value = datetime(2026, 1, 8, 9, 45, 0)
        mock_dt.time = time

        # Attempt to place order
        with pytest.raises(KillZoneViolationError):
            place_order(mock_order)
```

**Integration Test:**
```bash
# Try placing order at 9:35 AM
python scripts/run_paper_trade.py --cap 10
# Expected: KillZoneViolationError raised
# Actual order NOT sent to broker
```

### Rollback
- Feature flag: `--enforce-kill-zone` (default: True)
- Disable with `--no-kill-zone-enforcement` for testing

### Cost
- **FREE** - just wiring existing infrastructure

### ROI
- **Quant-grade safety** (hard enforcement like PolicyGate)
- **Prevents amateur hour trading** (9:30-10:00 AM)
- **Prevents lunch chop trading** (11:30-14:00 PM)

---

## FIX #6: Add Airbnb Feature Store Pattern

**Priority:** âš ï¸ MEDIUM (Week 2, Day 3-4)
**Category:** Wiring Fix (Data Quality)

### Problem
- 99 enrichment fields created but no schema enforcement
- No staleness checks (could use yesterday's sentiment)
- No versioning (cannot track feature pipeline changes)
- DecisionPacket not saved (see Fix #1)

### Impact
- Risk of stale features reaching execution
- Cannot detect feature pipeline drift
- Cannot roll back feature changes

### Files to Modify
- `ml_features/feature_store.py` (new file)
- `pipelines/unified_signal_enrichment.py` (add validation)

### Change

**New File:** `ml_features/feature_store.py`

```python
"""Airbnb-style feature store with schema enforcement and staleness checks."""
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import pandas as pd

@dataclass
class FeatureSchema:
    """Schema definition for feature."""
    name: str
    dtype: str  # float, int, str, bool
    required: bool = True
    valid_range: Optional[tuple] = None  # (min, max)
    staleness_threshold: timedelta = timedelta(hours=24)

class FeatureStore:
    """Feature store with validation and staleness checks."""

    SCHEMA = [
        FeatureSchema("ml_meta_conf", "float", required=True, valid_range=(0.0, 1.0)),
        FeatureSchema("lstm_direction", "float", required=False, valid_range=(0.0, 1.0)),
        FeatureSchema("sentiment_score", "float", required=True, valid_range=(-1.0, 1.0)),
        FeatureSchema("regime", "str", required=True),
        FeatureSchema("vix_level", "float", required=True, valid_range=(5.0, 100.0)),
        # ... all 99 fields
    ]

    def validate_features(self, features: Dict[str, Any]) -> tuple[bool, List[str]]:
        """Validate features against schema."""
        errors = []

        for schema in self.SCHEMA:
            # Check required fields
            if schema.required and schema.name not in features:
                errors.append(f"Missing required field: {schema.name}")
                continue

            if schema.name not in features:
                continue  # Optional field

            value = features[schema.name]

            # Check dtype
            expected_type = {'float': float, 'int': int, 'str': str, 'bool': bool}[schema.dtype]
            if not isinstance(value, expected_type):
                errors.append(f"{schema.name}: expected {schema.dtype}, got {type(value).__name__}")

            # Check valid range
            if schema.valid_range and isinstance(value, (int, float)):
                min_val, max_val = schema.valid_range
                if not (min_val <= value <= max_val):
                    errors.append(f"{schema.name}: {value} outside valid range [{min_val}, {max_val}]")

        return (len(errors) == 0, errors)

    def check_staleness(self, features: Dict[str, Any], feature_timestamp: datetime) -> Dict[str, bool]:
        """Check if features are stale."""
        now = datetime.utcnow()
        staleness = {}

        for schema in self.SCHEMA:
            if schema.name not in features:
                continue

            age = now - feature_timestamp
            staleness[schema.name] = (age > schema.staleness_threshold)

        return staleness
```

**Wire into Enrichment:**

```python
# Add validation stage
@dataclass
class FeatureValidation(EnrichmentStage):
    name: str = "feature_validation"

    def enrich(self, signal: EnrichedSignal) -> EnrichedSignal:
        """Validate features against schema."""
        store = FeatureStore()

        # Validate
        valid, errors = store.validate_features(signal.to_dict())

        if not valid:
            logger.warning(f"Feature validation failed for {signal.symbol}: {errors}")
            signal.feature_validation_passed = False
            signal.feature_validation_errors = errors
        else:
            signal.feature_validation_passed = True
            signal.feature_validation_errors = []

        # Check staleness
        staleness = store.check_staleness(signal.to_dict(), signal.timestamp)
        stale_features = [k for k, v in staleness.items() if v]

        if stale_features:
            logger.warning(f"Stale features for {signal.symbol}: {stale_features}")
            signal.has_stale_features = True
            signal.stale_features = stale_features
        else:
            signal.has_stale_features = False
            signal.stale_features = []

        return signal
```

### Verification

**Unit Test:**
```python
def test_feature_store_validation():
    """Verify feature store rejects invalid features."""
    store = FeatureStore()

    # Invalid: ml_meta_conf out of range
    features = {'ml_meta_conf': 1.5, 'sentiment_score': 0.5}
    valid, errors = store.validate_features(features)
    assert not valid
    assert any('outside valid range' in err for err in errors)

    # Invalid: missing required field
    features = {'ml_meta_conf': 0.7}  # Missing sentiment_score
    valid, errors = store.validate_features(features)
    assert not valid
    assert any('Missing required field: sentiment_score' in err for err in errors)
```

### Rollback
- Feature flag: `--validate-features` (default: True)
- Disable with `--no-feature-validation` for testing

### Cost
- **FREE** - just validation logic

### ROI
- **Data quality guarantees** (schema enforcement)
- **Staleness detection** (prevent using yesterday's data)
- **Debugging** (know exactly which features are invalid)

---

## FIX #7-10: Lower Priority Enhancements

### Fix #7: Add LLM Router (agentic-flow)
**Priority:** LOW (Week 3)
- Route simple tasks to Haiku ($0.25/1M tokens)
- Route complex tasks to Sonnet ($3/1M tokens)
- Claimed 60% cost savings (need validation)

### Fix #8: Upgrade to NextChat UI
**Priority:** LOW (Week 4)
- Replace Streamlit with NextChat PWA
- Add multi-user support with auth
- Cross-platform (desktop, mobile, tablet)

### Fix #9: Automate Dev Environment (Lissy93 dotfiles)
**Priority:** LOW (Week 4)
- Create `./install.sh` script
- Add Docker containerization
- Fresh checkout â†’ working system in < 10 minutes

### Fix #10: Clean Up Dead Code
**Priority:** MEDIUM (Week 2, Day 6-7)
- Archive RAG, Ensemble, Anomaly (dead code)
- Remove deprecated cognitive_brain
- Update imports and tests

---

## SUMMARY: PRIORITIZED ROADMAP

| Week | Fix | Priority | Effort | ROI |
|------|-----|----------|--------|-----|
| **1** | #1: Save DecisionPacket | ðŸ”´ CRITICAL | LOW | HIGH |
| **1** | #2: FinGPT Sentiment | ðŸ”´ HIGH | MEDIUM | HIGH |
| **1** | #3: TradeMaster PRUDEX | ðŸ”´ HIGH | LOW | HIGH |
| **2** | #5: Kill Zone Enforcement | âš ï¸ MEDIUM | LOW | MEDIUM |
| **2** | #6: Feature Store Pattern | âš ï¸ MEDIUM | MEDIUM | MEDIUM |
| **2** | #10: Clean Up Dead Code | âš ï¸ MEDIUM | LOW | MEDIUM |
| **3** | #4: Rebuild RAG | âš ï¸ MEDIUM | HIGH | MEDIUM |
| **3** | #7: LLM Router | âš ï¸ LOW | LOW | MEDIUM |
| **4** | #8: NextChat UI | âš ï¸ LOW | MEDIUM | LOW |
| **4** | #9: Dev Automation | âš ï¸ LOW | LOW | LOW |

---

## SUCCESS CRITERIA

| Metric | Current | Target (2 Weeks) | How to Measure |
|--------|---------|------------------|----------------|
| **DecisionPacket saved** | 0% | 100% | Check state/decisions/ directory |
| **Sentiment defaults (0.5)** | 30% | 0% | Count null sentiment scores |
| **RL benchmark** | None | PRUDEX report | Run TradeMaster evaluation |
| **Kill zone violations** | Possible | 0 | Test order at 9:35 AM |
| **Feature validation** | None | 100% | Count validation errors |
| **Dead code %** | 25% | <5% | grep for unused imports |

**Quant Interview Readiness:**
- âœ… Can cite PRUDEX benchmark for RL agent
- âœ… Can explain DecisionPacket for reproducibility
- âœ… Can demonstrate feature store pattern
- âœ… Can show sentiment integration with FinGPT
- âœ… Can prove risk gates are authoritative

