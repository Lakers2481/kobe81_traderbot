# RL Trading Agent Enhancement Guide

**Generated by researcher-prime on 2026-01-01**

## 1. Sharpe Ratio Reward Function (CRITICAL FIX)

Replace raw PnL reward with Sharpe ratio for stability:

```python
# ml/alpha_discovery/rl_agent/trading_env.py

import numpy as np
from collections import deque

class TradingEnv:
    def __init__(self, ...):
        self.returns_window = deque(maxlen=20)  # 20-day rolling window
        self.transaction_cost = 0.001  # 0.1% per trade

    def _calculate_reward(self, prev_portfolio_value, current_portfolio_value, trade_occurred):
        # Daily return
        daily_return = (current_portfolio_value - prev_portfolio_value) / prev_portfolio_value

        # Transaction cost penalty
        if trade_occurred:
            daily_return -= self.transaction_cost

        # Add to rolling window
        self.returns_window.append(daily_return)

        # Sharpe ratio reward (annualized)
        if len(self.returns_window) >= 5:  # Need minimum observations
            returns_array = np.array(self.returns_window)
            mean_return = np.mean(returns_array)
            std_return = np.std(returns_array)

            if std_return > 1e-6:  # Avoid division by zero
                sharpe = (mean_return / std_return) * np.sqrt(252)  # Annualized
            else:
                sharpe = 0.0
        else:
            sharpe = 0.0

        # Drawdown penalty (exponential scaling)
        current_drawdown = self._calculate_drawdown()
        if current_drawdown > 0.10:  # 10% threshold
            dd_penalty = -10.0 * (current_drawdown - 0.10) ** 2
        else:
            dd_penalty = 0.0

        return sharpe + dd_penalty

    def _calculate_drawdown(self):
        # Track peak portfolio value
        if not hasattr(self, 'peak_value'):
            self.peak_value = self.initial_portfolio_value

        self.peak_value = max(self.peak_value, self.current_portfolio_value)
        return (self.peak_value - self.current_portfolio_value) / self.peak_value
```

## 2. Enhanced State Representation

Add portfolio state, risk metrics, and regime features:

```python
# ml/alpha_discovery/rl_agent/trading_env.py

def _get_observation(self):
    # Price features (existing)
    price_features = self._get_price_features()  # OHLCV, RSI, ATR, etc.

    # Portfolio state (NEW)
    portfolio_features = np.array([
        self.current_position / self.max_position,  # Normalized position [-1, 1]
        self.cash / self.initial_cash,  # Cash ratio
        self.unrealized_pnl / self.initial_portfolio_value,  # Unrealized P&L %
        self._calculate_drawdown(),  # Current drawdown
    ])

    # Risk metrics (NEW - integrate with your risk/advanced/)
    from risk.advanced.monte_carlo_var import calculate_portfolio_var
    from risk.advanced.correlation_limits import get_correlation_matrix

    # Simplified VaR (95% confidence)
    recent_returns = list(self.returns_window)[-20:]
    var_95 = np.percentile(recent_returns, 5) if len(recent_returns) > 0 else 0.0

    risk_features = np.array([
        var_95,
        np.std(recent_returns) if len(recent_returns) > 0 else 0.0,  # Volatility
    ])

    # Regime features (NEW - integrate with your HMM detector)
    from ml_advanced.hmm_regime_detector import HMMRegimeDetector

    # Load regime detector (cache this in __init__)
    if not hasattr(self, 'regime_detector'):
        self.regime_detector = HMMRegimeDetector()
        # Load pre-trained model or train on initialization

    current_regime = self.regime_detector.predict_current_regime(price_features)
    regime_onehot = np.zeros(3)  # 3 states: bull, bear, neutral
    regime_onehot[current_regime] = 1.0

    # Concatenate all features
    obs = np.concatenate([
        price_features,
        portfolio_features,
        risk_features,
        regime_onehot,
    ])

    return obs
```

## 3. Continuous Action Space for Position Sizing

Replace discrete buy/sell/hold with continuous position sizing:

```python
# ml/alpha_discovery/rl_agent/trading_env.py

import gymnasium as gym

class TradingEnv(gym.Env):
    def __init__(self, ...):
        # Continuous action space: target position in [-1, 1]
        # -1 = max short, 0 = flat, +1 = max long
        self.action_space = gym.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(1,),
            dtype=np.float32
        )

    def step(self, action):
        target_position = float(action[0])  # In range [-1, 1]

        # Convert to dollar position
        max_position_value = self.current_portfolio_value * 0.95  # Leave 5% cash buffer
        target_position_dollars = target_position * max_position_value

        # Calculate required trade
        current_position_dollars = self.current_position * self.current_price
        trade_dollars = target_position_dollars - current_position_dollars

        # Execute trade (integrate with your execution layer)
        if abs(trade_dollars) > self.min_trade_size:
            trade_occurred = True
            shares_to_trade = trade_dollars / self.current_price

            # Apply transaction costs
            transaction_cost = abs(trade_dollars) * self.transaction_cost_pct
            self.cash -= transaction_cost

            # Update position
            self.current_position += shares_to_trade
            self.cash -= trade_dollars
        else:
            trade_occurred = False

        # Calculate reward
        reward = self._calculate_reward(prev_portfolio_value, current_portfolio_value, trade_occurred)

        # ... rest of step logic
```

## 4. CVaR-Constrained PPO (Risk-Aware Agent)

Add CVaR constraint using your existing VaR calculations:

```python
# ml/alpha_discovery/rl_agent/agent.py

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np

class CVaRConstraintCallback(BaseCallback):
    """
    Callback to penalize policies that exceed CVaR threshold.
    Integrates with risk/advanced/monte_carlo_var.py
    """
    def __init__(self, cvar_threshold=0.05, penalty_weight=10.0):
        super().__init__()
        self.cvar_threshold = cvar_threshold  # Max 5% CVaR
        self.penalty_weight = penalty_weight
        self.episode_returns = []

    def _on_step(self):
        # Collect episode returns
        if 'episode' in self.locals.get('infos', [{}])[0]:
            episode_return = self.locals['infos'][0]['episode']['r']
            self.episode_returns.append(episode_return)

            # Calculate CVaR (95% confidence)
            if len(self.episode_returns) >= 100:
                returns_array = np.array(self.episode_returns[-100:])
                cvar_95 = np.mean(returns_array[returns_array <= np.percentile(returns_array, 5)])

                # If CVaR exceeds threshold, add penalty to reward
                if abs(cvar_95) > self.cvar_threshold:
                    penalty = -self.penalty_weight * (abs(cvar_95) - self.cvar_threshold)

                    # Modify reward (this is a simplified approach)
                    # For proper implementation, modify reward in env or use constrained RL
                    self.logger.record("cvar/penalty", penalty)
                    self.logger.record("cvar/current", cvar_95)

        return True

# Usage in agent.py
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    verbose=1,
    tensorboard_log="./rl_logs/"
)

cvar_callback = CVaRConstraintCallback(cvar_threshold=0.05)
model.learn(total_timesteps=100000, callback=cvar_callback)
```

## 5. Offline RL Pre-Training (D3RLPY Integration)

Pre-train agent on your historical signals before live tuning:

```bash
pip install d3rlpy
```

```python
# scripts/pretrain_rl_offline.py

import pandas as pd
import numpy as np
from d3rlpy.algos import CQLConfig
from d3rlpy.dataset import create_fifo_replay_buffer
from d3rlpy.metrics import TDErrorEvaluator, DiscountedSumOfAdvantageEvaluator

# Load your historical signals
signals_df = pd.read_csv("logs/signals.jsonl")  # Your existing signal log

# Convert to offline RL dataset format
observations = []
actions = []
rewards = []
terminals = []

# ... (convert your signal history to trajectory format)

# Create replay buffer
dataset = create_fifo_replay_buffer(
    limit=100000,
    episodes=trajectories  # List of (obs, action, reward, terminal) tuples
)

# Configure Conservative Q-Learning (CQL) - prevents overestimation
cql = CQLConfig(
    actor_learning_rate=3e-4,
    critic_learning_rate=3e-4,
    batch_size=256,
).create()

# Train offline
cql.fit(
    dataset,
    n_steps=50000,
    evaluators={
        'td_error': TDErrorEvaluator(),
        'advantage': DiscountedSumOfAdvantageEvaluator(),
    }
)

# Save pre-trained model
cql.save_model("models/cql_pretrained.d3")

# Fine-tune online with PPO
# Load CQL weights into PPO policy network (manual weight transfer)
```

## 6. Multi-Asset Portfolio RL (FinRL Approach)

Extend to multi-asset using your universe:

```python
# ml/alpha_discovery/rl_agent/portfolio_env.py

import gymnasium as gym
import numpy as np

class MultiAssetPortfolioEnv(gym.Env):
    """
    Multi-asset portfolio environment.
    Action: Portfolio weights for N assets (softmax normalized to sum to 1).
    """
    def __init__(self, symbols, price_data_dict):
        self.symbols = symbols
        self.n_assets = len(symbols)
        self.price_data = price_data_dict  # {symbol: DataFrame}

        # Action space: portfolio weights [w1, w2, ..., wN]
        # Will be softmax normalized to sum to 1
        self.action_space = gym.spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.n_assets,),
            dtype=np.float32
        )

        # Observation: price features for all assets + portfolio state
        obs_dim = self.n_assets * 50 + 10  # 50 features per asset + 10 portfolio features
        self.observation_space = gym.spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32
        )

    def step(self, action):
        # Softmax normalize to get portfolio weights
        weights = self._softmax(action)

        # Ensure weights sum to 1 and are non-negative (long-only)
        weights = np.clip(weights, 0, 1)
        weights = weights / np.sum(weights)

        # Calculate returns for each asset
        asset_returns = []
        for symbol in self.symbols:
            current_price = self.price_data[symbol].iloc[self.current_step]['Close']
            prev_price = self.price_data[symbol].iloc[self.current_step - 1]['Close']
            asset_return = (current_price - prev_price) / prev_price
            asset_returns.append(asset_return)

        asset_returns = np.array(asset_returns)

        # Portfolio return (weighted average)
        portfolio_return = np.dot(weights, asset_returns)

        # Transaction cost (turnover penalty)
        prev_weights = self.current_weights
        turnover = np.sum(np.abs(weights - prev_weights))
        transaction_cost = turnover * 0.001  # 0.1% per unit turnover

        portfolio_return -= transaction_cost

        # Update portfolio value
        self.portfolio_value *= (1 + portfolio_return)
        self.current_weights = weights

        # Reward: Sharpe ratio (as before)
        reward = self._calculate_sharpe_reward(portfolio_return)

        # ... rest of step logic

        return obs, reward, done, truncated, info

    def _softmax(self, x):
        exp_x = np.exp(x - np.max(x))  # Numerical stability
        return exp_x / np.sum(exp_x)
```

## 7. Integration with Your Existing System

**Step-by-step integration plan:**

### Phase 1: Offline Pre-Training (Week 1)
1. Export your `logs/signals.jsonl` to offline RL dataset
2. Train CQL agent on historical signals (behavior cloning baseline)
3. Evaluate on validation set (2024 data)

### Phase 2: Environment Enhancement (Week 2)
1. Implement Sharpe ratio reward in `trading_env.py`
2. Add HMM regime state from `ml_advanced/hmm_regime_detector.py`
3. Add portfolio state and risk metrics
4. Switch to continuous action space

### Phase 3: Risk Integration (Week 3)
1. Implement CVaR constraint callback using `risk/advanced/monte_carlo_var.py`
2. Add correlation features from `risk/advanced/correlation_limits.py`
3. Integrate with `PolicyGate` for hard budget enforcement

### Phase 4: Online Fine-Tuning (Week 4)
1. Load pre-trained CQL weights into PPO agent
2. Fine-tune on paper trading data (limited online interaction)
3. Monitor distribution shift with `sentinel_audit_01`

### Phase 5: Multi-Asset Extension (Week 5+)
1. Implement `MultiAssetPortfolioEnv` for top 10 symbols from scanner
2. Train meta-agent for asset allocation
3. Use hierarchical RL: DualStrategyScanner selects candidates, RL agent allocates capital

## 8. Hyperparameter Tuning (Population-Based Training)

Use Ray Tune for automated hyperparameter search:

```python
# scripts/tune_rl_hyperparameters.py

from ray import tune
from ray.tune.schedulers import PopulationBasedTraining
from stable_baselines3 import PPO

def train_ppo(config):
    env = TradingEnv(...)

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=config["lr"],
        n_steps=config["n_steps"],
        batch_size=config["batch_size"],
        gamma=config["gamma"],
    )

    model.learn(total_timesteps=50000)

    # Evaluate on validation set
    val_sharpe = evaluate_agent(model, val_env)

    return {"sharpe": val_sharpe}

# Population-based training
pbt_scheduler = PopulationBasedTraining(
    time_attr="training_iteration",
    metric="sharpe",
    mode="max",
    perturbation_interval=5,
    hyperparam_mutations={
        "lr": tune.loguniform(1e-5, 1e-3),
        "gamma": tune.uniform(0.95, 0.999),
        "n_steps": [512, 1024, 2048],
        "batch_size": [64, 128, 256],
    },
)

tune.run(
    train_ppo,
    scheduler=pbt_scheduler,
    num_samples=20,  # 20 parallel trials
)
```

## 9. Safety Checks (Production Deployment)

Integrate RL agent with your existing safety systems:

```python
# execution/rl_executor.py

from ml.alpha_discovery.rl_agent.agent import load_trained_agent
from risk.policy_gate import PolicyGate
from cognitive.knowledge_boundary import KnowledgeBoundary

class RLExecutor:
    def __init__(self):
        self.rl_agent = load_trained_agent("models/ppo_finetuned.zip")
        self.policy_gate = PolicyGate()
        self.knowledge_boundary = KnowledgeBoundary()

    def get_rl_recommendation(self, obs):
        # Get RL agent action
        action, _states = self.rl_agent.predict(obs, deterministic=True)

        # Convert to trade recommendation
        target_position = float(action[0])  # In [-1, 1]

        # Check uncertainty (distributional RL or ensemble variance)
        uncertainty = self._estimate_uncertainty(obs)

        # If high uncertainty, route to knowledge boundary
        if uncertainty > 0.3:
            kb_decision = self.knowledge_boundary.evaluate(
                context={"obs": obs, "uncertainty": uncertainty}
            )
            if kb_decision.should_stand_down:
                return None  # No trade recommendation

        # Check against PolicyGate
        proposed_trade = {
            "symbol": "...",
            "side": "BUY" if target_position > 0 else "SELL",
            "quantity": abs(target_position) * 100,  # Convert to shares
            "limit_price": "...",
        }

        if not self.policy_gate.check(proposed_trade):
            return None  # Rejected by risk gate

        return proposed_trade

    def _estimate_uncertainty(self, obs):
        # Use ensemble disagreement or distributional RL
        # For simple approach, use dropout at inference time
        predictions = []
        for _ in range(10):
            action, _ = self.rl_agent.predict(obs, deterministic=False)
            predictions.append(float(action[0]))

        return np.std(predictions)
```

## 10. Monitoring & Drift Detection

Integrate with `sentinel_audit_01`:

```python
# Add to sentinel_audit_01 checks

def check_rl_agent_drift():
    """
    Monitor RL agent performance for concept drift.
    """
    recent_episodes = load_recent_rl_episodes(days=7)
    training_episodes = load_training_rl_episodes()

    # Compare recent vs training distribution
    recent_returns = [ep['return'] for ep in recent_episodes]
    training_returns = [ep['return'] for ep in training_episodes]

    # KS test for distribution shift
    from scipy.stats import ks_2samp
    ks_stat, p_value = ks_2samp(recent_returns, training_returns)

    if p_value < 0.05:
        return {
            "status": "WARNING",
            "message": f"RL agent distribution shift detected (KS p={p_value:.4f})",
            "recommendation": "Retrain agent on recent data"
        }

    return {"status": "OK"}
```

---

**PRIORITY IMPLEMENTATION ORDER:**
1. Sharpe ratio reward (immediate, critical fix)
2. Continuous action space (enables proper position sizing)
3. Portfolio state in observations (uses existing risk system)
4. HMM regime integration (leverages existing detector)
5. Offline pre-training (reduces overfitting risk)
6. CVaR constraints (production-ready risk management)
7. Multi-asset portfolio env (scalability)
8. Hyperparameter tuning with Ray (optimization)

**Next Steps:**
- Read FinRL docs: https://finrl.readthedocs.io/
- Clone FinRL repo: `git clone https://github.com/AI4Finance-Foundation/FinRL.git`
- Review TensorTrade reward schemes: https://github.com/tensortrade-org/tensortrade
- Experiment with D3RLPY offline RL: https://github.com/takuseno/d3rlpy
